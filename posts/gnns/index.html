<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Graph Neural Networks (GNNs) | Alireza's Blog</title><meta name=keywords content="GNN,GNNs,Deep Learning,Graph Theory,GCN,GraphSAGE,Machine Learning,DL,ML"><meta name=description content="This post is a summary of lectures 6 to 8 (Videos 17 to 25) of Stanford CS224W, &lsquo;Machine Learning with Graphs&rsquo; course. (youtube)
Introduction to Graph Neural Networks The goal is to find an encoding of vector $v$ based graph structure
$$ ENC(v) = \text{Multiple layers of non-linear transformation} \ \text{based on graph structure} $$
Deep Graph Encoder Tasks Node classification Link prediction Community detection Network similarity üòê Modern deep learning toolbox is designed for simple sequences and grids"><meta name=author content><link rel=canonical href=https://alirezanobakht13.github.io/posts/gnns/><link crossorigin=anonymous href=/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://alirezanobakht13.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://alirezanobakht13.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://alirezanobakht13.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://alirezanobakht13.github.io/apple-touch-icon.png><link rel=mask-icon href=https://alirezanobakht13.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Graph Neural Networks (GNNs)"><meta property="og:description" content="This post is a summary of lectures 6 to 8 (Videos 17 to 25) of Stanford CS224W, &lsquo;Machine Learning with Graphs&rsquo; course. (youtube)
Introduction to Graph Neural Networks The goal is to find an encoding of vector $v$ based graph structure
$$ ENC(v) = \text{Multiple layers of non-linear transformation} \ \text{based on graph structure} $$
Deep Graph Encoder Tasks Node classification Link prediction Community detection Network similarity üòê Modern deep learning toolbox is designed for simple sequences and grids"><meta property="og:type" content="article"><meta property="og:url" content="https://alirezanobakht13.github.io/posts/gnns/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-04-25T17:18:32+03:30"><meta property="article:modified_time" content="2023-04-25T17:18:32+03:30"><meta name=twitter:card content="summary"><meta name=twitter:title content="Graph Neural Networks (GNNs)"><meta name=twitter:description content="This post is a summary of lectures 6 to 8 (Videos 17 to 25) of Stanford CS224W, &lsquo;Machine Learning with Graphs&rsquo; course. (youtube)
Introduction to Graph Neural Networks The goal is to find an encoding of vector $v$ based graph structure
$$ ENC(v) = \text{Multiple layers of non-linear transformation} \ \text{based on graph structure} $$
Deep Graph Encoder Tasks Node classification Link prediction Community detection Network similarity üòê Modern deep learning toolbox is designed for simple sequences and grids"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://alirezanobakht13.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Graph Neural Networks (GNNs)","item":"https://alirezanobakht13.github.io/posts/gnns/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Graph Neural Networks (GNNs)","name":"Graph Neural Networks (GNNs)","description":"This post is a summary of lectures 6 to 8 (Videos 17 to 25) of Stanford CS224W, \u0026lsquo;Machine Learning with Graphs\u0026rsquo; course. (youtube)\nIntroduction to Graph Neural Networks The goal is to find an encoding of vector $v$ based graph structure\n$$ ENC(v) = \\text{Multiple layers of non-linear transformation} \\ \\text{based on graph structure} $$\nDeep Graph Encoder Tasks Node classification Link prediction Community detection Network similarity üòê Modern deep learning toolbox is designed for simple sequences and grids","keywords":["GNN","GNNs","Deep Learning","Graph Theory","GCN","GraphSAGE","Machine Learning","DL","ML"],"articleBody":"This post is a summary of lectures 6 to 8 (Videos 17 to 25) of Stanford CS224W, ‚ÄòMachine Learning with Graphs‚Äô course. (youtube)\nIntroduction to Graph Neural Networks The goal is to find an encoding of vector $v$ based graph structure\n$$ ENC(v) = \\text{Multiple layers of non-linear transformation} \\ \\text{based on graph structure} $$\nDeep Graph Encoder Tasks Node classification Link prediction Community detection Network similarity üòê Modern deep learning toolbox is designed for simple sequences and grids\nBut graphs:\nArbitrary size and complex topological structure No Fixed ordering or reference point Often dynamic and have multimodal features Basic of Deep Learning I skip this lecture.\nDeep Learning for Graphs Notion Description $G$ Graph $V$ Vertex set $A$ Adjacency matrix $X\\in \\mathbb{R}^{m\\times|V|}$ Matrix of node features $v\\in V$ A node $N(v)$ Set of neighbors of $v$ Naive approach Join adjacency matrix and feature and then feed them to the deep neural net:\nNaive Approach $O(|V|)$ parameters Not applicable to graphs with different size Depends on node ordering CNNs similar idea üí°Transform information at the neighbors and combine it.\nTransform messages $h_i$ from neighbors: $W_ih_i$ Add them up: $\\sum_iW_ih_i$ Graph Convolutional Neural Network (GCN) üí°Network neighborhood defines a computation graph\nComputation graph based on graph structure The math Initial 0th layer embeddings are equal to node feature $$h_v^0=x_v$$\nFor each layer $$ h_v^{(l+1)} = \\textcolor{orange}{\\sigma}( \\textcolor{pink}{W_l} \\textcolor{skyblue}{\\sum_{u\\in N(v)}\\frac{h_u^{(l)}}{|N(v)|}} + \\textcolor{pink}{B_l}h_v ), \\quad \\forall l \\in \\lbrace 0, \\dots, \\textcolor{purple}{L}-1 \\rbrace $$\nWhere: $\\textcolor{orange}{\\sigma}$: None-linearity (e.g. ReLU) $\\textcolor{pink}{W_l}$, $\\textcolor{pink}{B_l}$: Trainable weight matrix $\\textcolor{skyblue}{\\sum_{u\\in N(v)}\\frac{h_u^{(l)}}{|N(v)|}}$: Averages of neighbor‚Äôs previous layer embeddings $\\textcolor{purple}{L}$: Total number of layers Embedding after L layers of neighborhood aggregation $$z_v=h_v^{(L)}$$ Matrix formulation $H^{(l)}=[h_1^{(l)}\\dots h_{|V|}^{(l)}]^T$ Matrix form of hidden embedding $\\sum_{u\\in N(v)} h_u^{(l)} = A_{v,:}H^{(l)}$ $A_{v,:}$ means row $v$ of adjacency matrix $A$ $D$ is diagonal matrix where: $D_{v,v}=\\text{Deg}(v) + \\epsilon $ $D_{v,v}^{-1}=1/(|N(v)|+\\epsilon)$ $\\epsilon$ prevents dividing by zero. $\\tilde{A}=D^{-1}A$ Matrix form\n$$ H^{(l+1)}=\\sigma(\\textcolor{tomato}{\\tilde{A}H^{(l)}W_l^T } + \\textcolor{aqua}{H^{(l)}B_l^T}) $$\nNeighborhood aggregation Self transformation üôÉ Not all GNNs can be expressed in matrix form, when aggregation function is complex.\nHow to train a GNN Supervised Minimize loss between GNN output and label\n$$ \\min_{\\theta}\\mathcal{L}(y,f(z_v)) $$\nUnsupervised Because no label is available, use the graph structure as the supervision (e.g. node similarity based random walks, matrix factorization, etc.).\nInductive capacity The same aggregation parameters are shared for all nodes\nThe number of model parameters is sublinear in $|V|$ Generalize to unseen nodes A General GNN Framework Each topic will be discussed in next sections\nGeneral Framework of GNNs GNN Layer = Message + Aggregation Different instantiation under this perspective GCN, GraphSAGE, GAT, ‚Ä¶ Layer Connectivity Stack layers sequentially Ways of adding skip connection Graph Augmentation Graph feature augmentation Graph structure augmentation Learning Objective Supervised/Unsupervised objectives Node/Edge/Graph level objectives A Single Layer of GNN Message + Aggregation Framework Message $$ m_u^{(l)} = \\text{MSG}^{(l)}(h_u^{(l-1)}), \\quad u\\in \\lbrace N(v) \\cup v \\rbrace $$\nExample: A linear layer $m_u^{(l)}=W^{(l)}h_u^{(l-1)}$\n‚úçÔ∏è Note: Usually a different message computation is for neighbors and node $v$ itself. example:\n$m_u^{(l)}=\\textcolor{tomato}{W^{(l)}}h_u^{(l)}$ $m_v^{(l)}=\\textcolor{dodgerblue}{B^{(l)}}h_v^{(l)}$ Aggregation $$ h_v^{(l)} = \\text{AGG}^{(l)}(\\lbrace m_u^{l}, u \\in N(v) \\rbrace, m_v^{(l)}) $$\n‚ö†Ô∏è $\\text{AGG}^{(l)}$ should be an order invariant function (works on sets/multi-sets, not sequences)\nExample: Sum(.), Mean(.), Max(.), etc.\nPooling ones (like Max) are coordinate-wise. ‚úçÔ∏è Note: We can (should!) Add expressiveness using Nonlinearity\n$\\sigma(\\cdot)$: $\\text{Sigmoid}(\\cdot)$, $\\text{ReLU}(\\cdot)$, etc. Can be added to message or aggregation ‚¨áÔ∏è In the followings deepppink is message and orange is aggregation.\nGCN $$ h_v^{(l)} = \\textcolor{orange}{\\sigma( \\sum_{u\\in N(v)}} \\textcolor{deeppink}{W^{(l)}\\frac{h_u^{(l)}}{|N(v)|}} \\textcolor{orange}{)} $$\n‚úçÔ∏è In the GCN original paper they used different normalization rather than $1/|N(v)|$, link.\nGraphSAGE $$ h_v^{(l)} = \\textcolor{orange}{\\sigma( W^{(l)}\\cdot \\text{CONCAT(}} \\textcolor{deeppink}{h_v^{(l-1)}}, \\textcolor{orange}{\\text{AGG}(} \\lbrace\\textcolor{deeppink}{h_u^{(l-1)}}, \\forall u \\in N(v)\\rbrace \\textcolor{orange}{)))} $$\n‚úçÔ∏è Notes:\nTwo-stage aggregation is used: Aggregation from node neighbors the output of this stage is a message itself Further aggregation from the node itself $\\text{AGG}$ can be Mean, Pool, or apply LSTM to reshuffled of neighbors. (Optional) $l_2$ Normalization $h_v^{(l)} \\leftarrow h_v^{(l)}/||h_v^{(l)} ||_2$ In some cases results in performance improvement GAT (Graph ATtention) $$ h_v^{(l)} = \\textcolor{orange}{\\sigma( \\sum_{u\\in N(v)}} \\textcolor{deeppink}{\\alpha_{vu}W_V^{(l)}h_u^{(l-1)}} \\textcolor{orange}{)} $$\nWhere:\nAttention coefficient $e_{vu}$ (indicates importance of $u$‚Äôs message to node $v$) $$ e_{vu} = a(W_Q^{(l)}h_v^{(l-1)},W_K^{(l)}h_u^{(l-1)}) $$\nNormalize $e_{vu}$ to get attention weight $\\alpha_{vu}$ (using softmax) $$ \\alpha_{vu} = \\frac{\\text{exp}(e_{vu})}{\\sum_{k\\in N(v)}\\text{exp}{(e_{vk})}} $$\n‚úçÔ∏è Notes:\nAttention is inspired by cognitive attention In GCN/GraphSAGE: $\\alpha_{vu}=1/|N(v)|$ $a$ can be a simple single-layer neural network In attention terminology, $K$ is key, $Q$ is query, and $V$ is value. Multi-head attention Create multiple attention score $h_v^{(l)}[1] = \\sigma(\\sum_{u\\in N(v)}\\textcolor{red}{\\alpha_{vu}^1}W^{(l)}h_u^{(l-1)})$ $h_v^{(l)}[2] = \\sigma(\\sum_{u\\in N(v)}\\textcolor{green}{\\alpha_{vu}^2}W^{(l)}h_u^{(l-1)})$ $h_v^{(l)}[3] = \\sigma(\\sum_{u\\in N(v)}\\textcolor{blue}{\\alpha_{vu}^3}W^{(l)}h_u^{(l-1)})$ Aggregate (e.g. concatenation) $h_v^{(l)} = \\text{AGG}(h_v^{(l)}[1],h_v^{(l)}[2],h_v^{(l)}[3])$ Benefits of attention mechanism Computationally efficient Storage efficient Localized Inductive capability GNN Layer in Practice Many deep learning techniques can be used here, I just mention a few of them:\nBatch Normalization Stabilize neural network training Dropout Prevent over-fitting Attention/Gating Control the importance of a message Stacking Multiple Layers of GNNs Over-smoothing problem Link\nAfter Adding multiple layers of GNNs sequentially, all nodes converge to similar embedding which makes them hard to differentiate. It‚Äôs because of the Receptive field overlap in the depth layer.\nsolution Be cautious when adding layers: link Make aggregation/transformation become a deep neural network. link Add layers that don‚Äôt pass a message (MLP layers for pre-processing and post-processing). link Add skip connection in GNNs: link How to apply: link Other methods of skip connection: link Graph Augmentation for GNNs Our assumption so far\nRaw input graph = Computation graph\nReasons for breaking this assumption\nFeatures The input graph lacks features $\\rightarrow$ Feature augmentation Graph structure The graph is too sparse $\\rightarrow$ Add virtual nodes / edges The graph is too dense $\\rightarrow$ Sample neighbors when doing message passing The graph is too large $\\rightarrow$ Sample subgraphs to compute embedding Feature augmentation Used when graph does not have node features (common when we only have adjacency matrix)\nStandard approaches Constant value Unique ID to nodes should be converted to one-hot vectors Comparison constant vs one-hot node features ‚ö†Ô∏è Some structures are hard to learn by GNNs Example: Cycle count feature\ncycle count feature GNN can‚Äôt learn the length of a cycle that $v_1$ resides in\nTherefore, we should augment node features with cycle count manually by adding a vector of cycle counts like this:\n$$ \\text{cycle count feature: } [0, 0, 0, 1, 0, 0] $$\nwhere index 0 indicates that $v_1$ resides in cycle of length $0$, index 1 indicates cycle of length $1$, and so on.\nCommonly used augmented features Node Degree PageRank Clustering coefficient Node centrality Eigenvector Betweenness Closeness ‚Ä¶ Graphlet ‚Ä¶ Add virtual nodes / edges Virtual edge Common approach: connect 2-hop neighbors\ninstead of using adj. matrix $A$ for GNN computation, use $A^2 + A$ Use cases:\nBipartite graphs Virtual node Add a virtual node and connect it to other nodes (all or some of them)\nBenefits:\nGreatly improves message passing in sparse graphs Node neighborhood sampling Instead of using all nodes for message passing, (randomly) sample a node‚Äôs neighborhood. for example, if a node has 5 neighbors, sample 2 of them in the message passing phase.\nBenefits:\nReduce computational cost Allows for scaling to large graphs in practice it works great üëå Prediction with GNNs GNN training pipeline GNN training pipeline GNN output GNN output is a set of node embeddings:\n$$ \\lbrace h_v^{(L)}, \\forall v \\in G \\rbrace $$\nPrediction head Node-level tasks Edge-level tasks Graph-level tasks Node-level We can directly use node embeddings or transform them into label space ($y_v$ is ground truth label and $\\widehat{y}_v$ is model output):\n$$ \\textcolor{Chartreuse}{\\widehat{y}_v = h_v^{(L)}} $$\n$$ \\text{or} $$\n$$ \\textcolor{Chartreuse}{\\widehat{y}_v = Head_{node}(h_v^{(L)}) = W^{(H)}h_v^{(L)}} $$\nEdge-level $$ \\color{Chartreuse} \\widehat{y}_{uv} = Head_{edge}(h_u^{(L)},h_v^{(L)}) $$\nOptions for $Head_{edge}$:\nConcatenation + Linear: $\\text{Linear}(\\text{Concat}(h_u^{(L)},h_v^{(L)}))$ Similar to graph attention Dot product: $(h_u^{(L)})^T h_v^{(L)}$ This approach only applies to 1-way prediction Applying to $k$-way prediction Similar to multi-head attention: $W^{(1)}, ‚Ä¶, W^{(K)}$ is trainable $$ \\widehat{y}_{uv}^{(1)} = (h_u^{(L)})^T \\textcolor{red}{W^{(1)}}h_v^{(L)} $$\n$$ \\dots $$\n$$ \\widehat{y}_{uv}^{(K)} = (h_u^{(L)})^T \\textcolor{red}{W^{(K)}}h_v^{(L)} $$\n$$ \\widehat{y}_{uv} = \\text{Concat}(\\widehat{y}_{uv}^{(1)}, \\dots, \\widehat{y}_{uv}^{(K)}) \\in \\mathbb{R}^k $$\nGraph-level $$ \\color{Chartreuse} \\widehat{y}_G = Head_{graph}(\\lbrace h_v^{(L)} \\in \\mathbb{R}^d, \\forall v \\in G \\rbrace) $$\nOptions for $Head_{graph}$:\nGlobal mean pooling Global max pooling Global sum pooling These global pooling methods will work great over small graphs but\n‚ö†Ô∏è Global pooling over a (large) graph will lose information\nHierarchical global pooling Hierarchically pool section of nodes.\nWhich section?\nDiffpool idea:\nDiffpool Leverage 2 independent GNNs at each level GNN A: Compute node embeddings GNN B: Compute the cluster that a node belongs to GNNs A and B at each level can be executed in parallel GNNs A and B are trained jointly Training GNNs Setting-up GNN Prediction Tasks When Things Don‚Äôt Go As Planned The three above topics are almost the same in other deep learning areas so I will put them aside for now.\n","wordCount":"1463","inLanguage":"en","datePublished":"2023-04-25T17:18:32+03:30","dateModified":"2023-04-25T17:18:32+03:30","mainEntityOfPage":{"@type":"WebPage","@id":"https://alirezanobakht13.github.io/posts/gnns/"},"publisher":{"@type":"Organization","name":"Alireza's Blog","logo":{"@type":"ImageObject","url":"https://alirezanobakht13.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://alirezanobakht13.github.io/ accesskey=h title="Alireza's Blog (Alt + H)">Alireza's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://alirezanobakht13.github.io/about_me/ title="About Me"><span>About Me</span></a></li><li><a href=https://alirezanobakht13.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://alirezanobakht13.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://alirezanobakht13.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://alirezanobakht13.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://alirezanobakht13.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://alirezanobakht13.github.io/posts/>Posts</a></div><h1 class=post-title>Graph Neural Networks (GNNs)</h1><div class=post-meta><span title='2023-04-25 17:18:32 +0330 +0330'>April 25, 2023</span>&nbsp;¬∑&nbsp;7 min&nbsp;¬∑&nbsp;1463 words</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction-to-graph-neural-networks aria-label="Introduction to Graph Neural Networks">Introduction to Graph Neural Networks</a><ul><li><a href=#tasks aria-label=Tasks>Tasks</a></li></ul></li><li><a href=#basic-of-deep-learning aria-label="Basic of Deep Learning">Basic of Deep Learning</a></li><li><a href=#deep-learning-for-graphs aria-label="Deep Learning for Graphs">Deep Learning for Graphs</a><ul><li><a href=#naive-approach aria-label="Naive approach">Naive approach</a></li><li><a href=#cnns-similar-idea aria-label="CNNs similar idea">CNNs similar idea</a></li><li><a href=#graph-convolutional-neural-network-gcn aria-label="Graph Convolutional Neural Network (GCN)">Graph Convolutional Neural Network (GCN)</a></li><li><a href=#the-math aria-label="The math">The math</a><ul><li><a href=#matrix-formulation aria-label="Matrix formulation">Matrix formulation</a></li></ul></li><li><a href=#how-to-train-a-gnn aria-label="How to train a GNN">How to train a GNN</a><ul><li><a href=#supervised aria-label=Supervised>Supervised</a></li><li><a href=#unsupervised aria-label=Unsupervised>Unsupervised</a></li></ul></li><li><a href=#inductive-capacity aria-label="Inductive capacity">Inductive capacity</a></li></ul></li><li><a href=#a-general-gnn-framework aria-label="A General GNN Framework">A General GNN Framework</a><ul><li><a href=#gnn-layer--message--aggregation aria-label="GNN Layer = Message + Aggregation">GNN Layer = Message + Aggregation</a></li><li><a href=#layer-connectivity aria-label="Layer Connectivity">Layer Connectivity</a></li><li><a href=#graph-augmentation aria-label="Graph Augmentation">Graph Augmentation</a></li><li><a href=#learning-objective aria-label="Learning Objective">Learning Objective</a></li></ul></li><li><a href=#a-single-layer-of-gnn aria-label="A Single Layer of GNN">A Single Layer of GNN</a><ul><li><a href=#message--aggregation-framework aria-label="Message + Aggregation Framework">Message + Aggregation Framework</a><ul><li><a href=#message aria-label=Message>Message</a></li><li><a href=#aggregation aria-label=Aggregation>Aggregation</a></li></ul></li><li><a href=#gcn aria-label=GCN>GCN</a></li><li><a href=#graphsage aria-label=GraphSAGE>GraphSAGE</a></li><li><a href=#gat-graph-attention aria-label="GAT (Graph ATtention)">GAT (Graph ATtention)</a><ul><li><a href=#multi-head-attention aria-label="Multi-head attention">Multi-head attention</a></li><li><a href=#benefits-of-attention-mechanism aria-label="Benefits of attention mechanism">Benefits of attention mechanism</a></li></ul></li><li><a href=#gnn-layer-in-practice aria-label="GNN Layer in Practice">GNN Layer in Practice</a></li></ul></li><li><a href=#stacking-multiple-layers-of-gnns aria-label="Stacking Multiple Layers of GNNs">Stacking Multiple Layers of GNNs</a><ul><li><a href=#over-smoothing-problem aria-label="Over-smoothing problem">Over-smoothing problem</a></li><li><a href=#solution aria-label=solution>solution</a></li></ul></li><li><a href=#graph-augmentation-for-gnns aria-label="Graph Augmentation for GNNs">Graph Augmentation for GNNs</a><ul><li><a href=#feature-augmentation aria-label="Feature augmentation">Feature augmentation</a><ul><li><a href=#standard-approaches aria-label="Standard approaches">Standard approaches</a></li><li><a href=#comparison aria-label=Comparison>Comparison</a></li><li><a href=#-some-structures-are-hard-to-learn-by-gnns aria-label="‚ö†Ô∏è Some structures are hard to learn by GNNs">‚ö†Ô∏è Some structures are hard to learn by GNNs</a></li><li><a href=#commonly-used-augmented-features aria-label="Commonly used augmented features">Commonly used augmented features</a></li></ul></li><li><a href=#add-virtual-nodes--edges aria-label="Add virtual nodes / edges">Add virtual nodes / edges</a><ul><li><a href=#virtual-edge aria-label="Virtual edge">Virtual edge</a></li><li><a href=#virtual-node aria-label="Virtual node">Virtual node</a></li></ul></li><li><a href=#node-neighborhood-sampling aria-label="Node neighborhood sampling">Node neighborhood sampling</a></li></ul></li><li><a href=#prediction-with-gnns aria-label="Prediction with GNNs">Prediction with GNNs</a><ul><li><a href=#gnn-training-pipeline aria-label="GNN training pipeline">GNN training pipeline</a><ul><li><a href=#gnn-output aria-label="GNN output">GNN output</a></li></ul></li><li><a href=#prediction-head aria-label="Prediction head">Prediction head</a><ul><li><a href=#node-level aria-label=Node-level>Node-level</a></li><li><a href=#edge-level aria-label=Edge-level>Edge-level</a></li><li><a href=#graph-level aria-label=Graph-level>Graph-level</a><ul><li><a href=#hierarchical-global-pooling aria-label="Hierarchical global pooling">Hierarchical global pooling</a></li></ul></li></ul></li></ul></li><li><a href=#training-gnns aria-label="Training GNNs">Training GNNs</a></li><li><a href=#setting-up-gnn-prediction-tasks aria-label="Setting-up GNN Prediction Tasks">Setting-up GNN Prediction Tasks</a></li><li><a href=#when-things-dont-go-as-planned aria-label="When Things Don&amp;rsquo;t Go As Planned">When Things Don&rsquo;t Go As Planned</a></li></ul></div></details></div><div class=post-content><p>This post is a summary of lectures 6 to 8 (Videos 17 to 25) of Stanford CS224W, &lsquo;<em>Machine Learning with Graphs</em>&rsquo; course. (<a href="https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn">youtube</a>)</p><h2 id=introduction-to-graph-neural-networks>Introduction to Graph Neural Networks<a hidden class=anchor aria-hidden=true href=#introduction-to-graph-neural-networks>#</a></h2><p>The goal is to find an <em><strong>encoding</strong></em> of vector $v$ based graph structure</p><p>$$
ENC(v) = \text{Multiple layers of non-linear transformation} \ \text{based on graph structure}
$$</p><p><figure><img src=/img/GNNs/deep_graph_encoder.png alt="Deep Graph Encoder" title="Deep Graph Encoder"><figcaption style="text-align:center;width:75%;margin:10px auto 35px;line-height:.5;color:#888;font-size:15px;font-weight:400">Deep Graph Encoder</figcaption></figure></p><h3 id=tasks>Tasks<a hidden class=anchor aria-hidden=true href=#tasks>#</a></h3><ul><li>Node classification</li><li>Link prediction</li><li>Community detection</li><li>Network similarity</li></ul><p>üòê Modern deep learning toolbox is designed for simple sequences and grids</p><p>But graphs:</p><ul><li>Arbitrary size and complex topological structure</li><li>No Fixed ordering or reference point</li><li>Often dynamic and have multimodal features</li></ul><h2 id=basic-of-deep-learning>Basic of Deep Learning<a hidden class=anchor aria-hidden=true href=#basic-of-deep-learning>#</a></h2><p>I skip this lecture.</p><h2 id=deep-learning-for-graphs>Deep Learning for Graphs<a hidden class=anchor aria-hidden=true href=#deep-learning-for-graphs>#</a></h2><table><thead><tr><th>Notion</th><th>Description</th></tr></thead><tbody><tr><td>$G$</td><td>Graph</td></tr><tr><td>$V$</td><td>Vertex set</td></tr><tr><td>$A$</td><td>Adjacency matrix</td></tr><tr><td>$X\in \mathbb{R}^{m\times|V|}$</td><td>Matrix of node features</td></tr><tr><td>$v\in V$</td><td>A node</td></tr><tr><td>$N(v)$</td><td>Set of neighbors of $v$</td></tr></tbody></table><h3 id=naive-approach>Naive approach<a hidden class=anchor aria-hidden=true href=#naive-approach>#</a></h3><p>Join adjacency matrix and feature and then feed them to the deep neural net:</p><p><figure><img src=/img/GNNs/naive_approach.png alt="naive approach" title="naive approach"><figcaption style="text-align:center;width:75%;margin:10px auto 35px;line-height:.5;color:#888;font-size:15px;font-weight:400">Naive Approach</figcaption></figure></p><ul><li>$O(|V|)$ parameters</li><li>Not applicable to graphs with different size</li><li>Depends on node ordering</li></ul><h3 id=cnns-similar-idea>CNNs similar idea<a hidden class=anchor aria-hidden=true href=#cnns-similar-idea>#</a></h3><p>üí°Transform information at the neighbors and combine it.</p><ul><li>Transform messages $h_i$ from neighbors: $W_ih_i$</li><li>Add them up: $\sum_iW_ih_i$</li></ul><h3 id=graph-convolutional-neural-network-gcn>Graph Convolutional Neural Network (GCN)<a hidden class=anchor aria-hidden=true href=#graph-convolutional-neural-network-gcn>#</a></h3><p>üí°Network neighborhood defines a computation graph</p><p><figure><img src=/img/GNNs/gnn_computation_graph.png alt="Computation graph based on graph structure" title="Computation graph based on graph structure"><figcaption style="text-align:center;width:75%;margin:10px auto 35px;line-height:.5;color:#888;font-size:15px;font-weight:400">Computation graph based on graph structure</figcaption></figure></p><h3 id=the-math>The math<a hidden class=anchor aria-hidden=true href=#the-math>#</a></h3><ol><li>Initial 0th layer embeddings are equal to node feature</li></ol><p>$$h_v^0=x_v$$</p><ol start=2><li>For each layer</li></ol><p>$$
h_v^{(l+1)} =
\textcolor{orange}{\sigma}(
\textcolor{pink}{W_l}
\textcolor{skyblue}{\sum_{u\in N(v)}\frac{h_u^{(l)}}{|N(v)|}} +
\textcolor{pink}{B_l}h_v
), \quad \forall l \in \lbrace 0, \dots, \textcolor{purple}{L}-1 \rbrace
$$</p><ul><li>Where:<ul><li>$\textcolor{orange}{\sigma}$: None-linearity (e.g. ReLU)</li><li>$\textcolor{pink}{W_l}$, $\textcolor{pink}{B_l}$: Trainable weight matrix</li><li>$\textcolor{skyblue}{\sum_{u\in N(v)}\frac{h_u^{(l)}}{|N(v)|}}$: Averages of neighbor&rsquo;s previous layer embeddings</li><li>$\textcolor{purple}{L}$: Total number of layers</li></ul></li></ul><ol start=3><li>Embedding after L layers of neighborhood aggregation
$$z_v=h_v^{(L)}$$</li></ol><h4 id=matrix-formulation>Matrix formulation<a hidden class=anchor aria-hidden=true href=#matrix-formulation>#</a></h4><ul><li>$H^{(l)}=[h_1^{(l)}\dots h_{|V|}^{(l)}]^T$</li></ul><p><figure><img src=/img/GNNs/matrix_form_hidden_embedding.png alt="Matrix form of hidden embedding" title="Matrix form of hidden embedding"><figcaption style="text-align:center;width:75%;margin:10px auto 35px;line-height:.5;color:#888;font-size:15px;font-weight:400">Matrix form of hidden embedding</figcaption></figure></p><ul><li>$\sum_{u\in N(v)} h_u^{(l)} = A_{v,:}H^{(l)}$<ul><li>$A_{v,:}$ means row $v$ of adjacency matrix $A$</li></ul></li><li>$D$ is diagonal matrix where: $D_{v,v}=\text{Deg}(v) + \epsilon
$<ul><li>$D_{v,v}^{-1}=1/(|N(v)|+\epsilon)$</li><li>$\epsilon$ prevents dividing by zero.</li></ul></li><li>$\tilde{A}=D^{-1}A$</li></ul><p>Matrix form</p><p>$$
H^{(l+1)}=\sigma(\textcolor{tomato}{\tilde{A}H^{(l)}W_l^T } + \textcolor{aqua}{H^{(l)}B_l^T})
$$</p><ul><li><span style=color:tomato>Neighborhood aggregation</span></li><li><span style=color:aqua>Self transformation</span></li></ul><p>üôÉ Not all GNNs can be expressed in matrix form, when aggregation function is complex.</p><h3 id=how-to-train-a-gnn>How to train a GNN<a hidden class=anchor aria-hidden=true href=#how-to-train-a-gnn>#</a></h3><h4 id=supervised>Supervised<a hidden class=anchor aria-hidden=true href=#supervised>#</a></h4><p>Minimize loss between GNN output and label</p><p>$$
\min_{\theta}\mathcal{L}(y,f(z_v))
$$</p><h4 id=unsupervised>Unsupervised<a hidden class=anchor aria-hidden=true href=#unsupervised>#</a></h4><p>Because no label is available, use the graph structure as the supervision (e.g. node similarity based random walks, matrix factorization, etc.).</p><h3 id=inductive-capacity>Inductive capacity<a hidden class=anchor aria-hidden=true href=#inductive-capacity>#</a></h3><p>The same aggregation parameters are shared for all nodes</p><ul><li>The number of model parameters is sublinear in $|V|$</li><li>Generalize to unseen nodes</li></ul><h2 id=a-general-gnn-framework>A General GNN Framework<a hidden class=anchor aria-hidden=true href=#a-general-gnn-framework>#</a></h2><p>Each topic will be discussed in next sections</p><p><figure><img src=/img/GNNs/general_framework_gnn.png alt="general framework of GNNs" title="general framework of GNNs"><figcaption style="text-align:center;width:75%;margin:10px auto 35px;line-height:.5;color:#888;font-size:15px;font-weight:400">General Framework of GNNs</figcaption></figure></p><h3 id=gnn-layer--message--aggregation>GNN Layer = Message + Aggregation<a hidden class=anchor aria-hidden=true href=#gnn-layer--message--aggregation>#</a></h3><ul><li>Different instantiation under this perspective</li><li>GCN, GraphSAGE, GAT, &mldr;</li></ul><h3 id=layer-connectivity>Layer Connectivity<a hidden class=anchor aria-hidden=true href=#layer-connectivity>#</a></h3><ul><li>Stack layers sequentially</li><li>Ways of adding skip connection</li></ul><h3 id=graph-augmentation>Graph Augmentation<a hidden class=anchor aria-hidden=true href=#graph-augmentation>#</a></h3><ul><li>Graph feature augmentation</li><li>Graph structure augmentation</li></ul><h3 id=learning-objective>Learning Objective<a hidden class=anchor aria-hidden=true href=#learning-objective>#</a></h3><ul><li>Supervised/Unsupervised objectives</li><li>Node/Edge/Graph level objectives</li></ul><h2 id=a-single-layer-of-gnn>A Single Layer of GNN<a hidden class=anchor aria-hidden=true href=#a-single-layer-of-gnn>#</a></h2><h3 id=message--aggregation-framework>Message + Aggregation Framework<a hidden class=anchor aria-hidden=true href=#message--aggregation-framework>#</a></h3><h4 id=message>Message<a hidden class=anchor aria-hidden=true href=#message>#</a></h4><p>$$
m_u^{(l)} = \text{MSG}^{(l)}(h_u^{(l-1)}), \quad u\in \lbrace N(v) \cup v \rbrace
$$</p><p>Example: A linear layer $m_u^{(l)}=W^{(l)}h_u^{(l-1)}$</p><p>‚úçÔ∏è <strong>Note</strong>: Usually a different message computation is for neighbors and node $v$ itself. example:</p><ul><li>$m_u^{(l)}=\textcolor{tomato}{W^{(l)}}h_u^{(l)}$</li><li>$m_v^{(l)}=\textcolor{dodgerblue}{B^{(l)}}h_v^{(l)}$</li></ul><h4 id=aggregation>Aggregation<a hidden class=anchor aria-hidden=true href=#aggregation>#</a></h4><p>$$
h_v^{(l)} = \text{AGG}^{(l)}(\lbrace m_u^{l}, u \in N(v) \rbrace, m_v^{(l)})
$$</p><p>‚ö†Ô∏è $\text{AGG}^{(l)}$ should be an <em><strong>order invariant</strong></em> function (works on sets/multi-sets, not sequences)</p><p>Example: Sum(.), Mean(.), Max(.), etc.</p><ul><li>Pooling ones (like Max) are coordinate-wise.</li></ul><p>‚úçÔ∏è <strong>Note</strong>: We can (should!) Add expressiveness using <em><strong>Nonlinearity</strong></em></p><ul><li>$\sigma(\cdot)$: $\text{Sigmoid}(\cdot)$, $\text{ReLU}(\cdot)$, etc.</li><li>Can be added to <em>message</em> or <em>aggregation</em></li></ul><p>‚¨áÔ∏è In the followings <span style=color:#ff1493>deepppink is message</span> and <span style=color:orange>orange is aggregation</span>.</p><h3 id=gcn>GCN<a hidden class=anchor aria-hidden=true href=#gcn>#</a></h3><p>$$
h_v^{(l)} = \textcolor{orange}{\sigma(
\sum_{u\in N(v)}}
\textcolor{deeppink}{W^{(l)}\frac{h_u^{(l)}}{|N(v)|}}
\textcolor{orange}{)}
$$</p><p>‚úçÔ∏è In the GCN original paper they used different normalization rather than $1/|N(v)|$, <a href=https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html#torch_geometric.nn.conv.GCNConv>link</a>.</p><h3 id=graphsage>GraphSAGE<a hidden class=anchor aria-hidden=true href=#graphsage>#</a></h3><p>$$
h_v^{(l)} = \textcolor{orange}{\sigma(
W^{(l)}\cdot \text{CONCAT(}}
\textcolor{deeppink}{h_v^{(l-1)}},
\textcolor{orange}{\text{AGG}(}
\lbrace\textcolor{deeppink}{h_u^{(l-1)}}, \forall u \in N(v)\rbrace
\textcolor{orange}{)))}
$$</p><p>‚úçÔ∏è Notes:</p><ul><li>Two-stage aggregation is used:<ul><li>Aggregation from node neighbors<ul><li>the output of this stage is a message itself</li></ul></li><li>Further aggregation from the node itself</li></ul></li><li>$\text{AGG}$ can be Mean, Pool, or apply LSTM to reshuffled of neighbors.</li><li>(Optional) $l_2$ Normalization<ul><li>$h_v^{(l)} \leftarrow h_v^{(l)}/||h_v^{(l)} ||_2$</li><li>In some cases results in performance improvement</li></ul></li></ul><h3 id=gat-graph-attention>GAT (Graph ATtention)<a hidden class=anchor aria-hidden=true href=#gat-graph-attention>#</a></h3><p>$$
h_v^{(l)} = \textcolor{orange}{\sigma(
\sum_{u\in N(v)}}
\textcolor{deeppink}{\alpha_{vu}W_V^{(l)}h_u^{(l-1)}}
\textcolor{orange}{)}
$$</p><p>Where:</p><ol><li>Attention coefficient $e_{vu}$ (<strong>indicates importance of $u$&rsquo;s message to node $v$</strong>)</li></ol><p>$$
e_{vu} = a(W_Q^{(l)}h_v^{(l-1)},W_K^{(l)}h_u^{(l-1)})
$$</p><ol start=2><li>Normalize $e_{vu}$ to get attention weight $\alpha_{vu}$ (using softmax)</li></ol><p>$$
\alpha_{vu} = \frac{\text{exp}(e_{vu})}{\sum_{k\in N(v)}\text{exp}{(e_{vk})}}
$$</p><p>‚úçÔ∏è Notes:</p><ul><li>Attention is inspired by cognitive attention</li><li>In GCN/GraphSAGE: $\alpha_{vu}=1/|N(v)|$</li><li>$a$ can be a simple single-layer neural network</li><li>In attention terminology, $K$ is key, $Q$ is query, and $V$ is value.</li></ul><h4 id=multi-head-attention>Multi-head attention<a hidden class=anchor aria-hidden=true href=#multi-head-attention>#</a></h4><ul><li>Create multiple attention score<ul><li>$h_v^{(l)}[1] = \sigma(\sum_{u\in N(v)}\textcolor{red}{\alpha_{vu}^1}W^{(l)}h_u^{(l-1)})$</li><li>$h_v^{(l)}[2] = \sigma(\sum_{u\in N(v)}\textcolor{green}{\alpha_{vu}^2}W^{(l)}h_u^{(l-1)})$</li><li>$h_v^{(l)}[3] = \sigma(\sum_{u\in N(v)}\textcolor{blue}{\alpha_{vu}^3}W^{(l)}h_u^{(l-1)})$</li></ul></li><li>Aggregate (e.g. concatenation)<ul><li>$h_v^{(l)} = \text{AGG}(h_v^{(l)}[1],h_v^{(l)}[2],h_v^{(l)}[3])$</li></ul></li></ul><h4 id=benefits-of-attention-mechanism>Benefits of attention mechanism<a hidden class=anchor aria-hidden=true href=#benefits-of-attention-mechanism>#</a></h4><ul><li>Computationally efficient</li><li>Storage efficient</li><li>Localized</li><li>Inductive capability</li></ul><h3 id=gnn-layer-in-practice>GNN Layer in Practice<a hidden class=anchor aria-hidden=true href=#gnn-layer-in-practice>#</a></h3><p>Many deep learning techniques can be used here, I just mention a few of them:</p><ul><li>Batch Normalization<ul><li>Stabilize neural network training</li></ul></li><li>Dropout<ul><li>Prevent over-fitting</li></ul></li><li>Attention/Gating<ul><li>Control the importance of a message</li></ul></li></ul><h2 id=stacking-multiple-layers-of-gnns>Stacking Multiple Layers of GNNs<a hidden class=anchor aria-hidden=true href=#stacking-multiple-layers-of-gnns>#</a></h2><h3 id=over-smoothing-problem>Over-smoothing problem<a hidden class=anchor aria-hidden=true href=#over-smoothing-problem>#</a></h3><p><a href="https://youtu.be/ew1cnUjRgl4?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&amp;t=165">Link</a></p><p>After Adding multiple layers of GNNs sequentially, all nodes converge to similar embedding which makes them hard to differentiate. It&rsquo;s because of the <em>Receptive field</em> overlap in the depth layer.</p><h3 id=solution>solution<a hidden class=anchor aria-hidden=true href=#solution>#</a></h3><ul><li>Be cautious when adding layers: <a href="https://youtu.be/ew1cnUjRgl4?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&amp;t=478">link</a><ul><li>Make aggregation/transformation become a deep neural network. <a href="https://youtu.be/ew1cnUjRgl4?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&amp;t=564">link</a></li><li>Add layers that don&rsquo;t pass a message (MLP layers for pre-processing and post-processing). <a href="https://youtu.be/ew1cnUjRgl4?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&amp;t=624">link</a></li></ul></li><li>Add skip connection in GNNs: <a href="https://youtu.be/ew1cnUjRgl4?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&amp;t=722">link</a><ul><li>How to apply: <a href="https://youtu.be/ew1cnUjRgl4?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&amp;t=949">link</a></li><li>Other methods of skip connection: <a href="https://youtu.be/ew1cnUjRgl4?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&amp;t=1002">link</a></li></ul></li></ul><h2 id=graph-augmentation-for-gnns>Graph Augmentation for GNNs<a hidden class=anchor aria-hidden=true href=#graph-augmentation-for-gnns>#</a></h2><p>Our assumption so far</p><blockquote><p>Raw input graph = Computation graph</p></blockquote><p>Reasons for breaking this assumption</p><ul><li>Features<ul><li>The input graph <span style=color:red><strong>lacks features</strong></span><ul><li>$\rightarrow$ <span style=color:#7fff00><strong>Feature augmentation</strong></span></li></ul></li></ul></li><li>Graph structure<ul><li>The graph is <span style=color:red><strong>too sparse</strong></span><ul><li>$\rightarrow$ <span style=color:#7fff00><strong>Add virtual nodes / edges</strong></span></li></ul></li><li>The graph is <span style=color:red><strong>too dense</strong></span><ul><li>$\rightarrow$ <span style=color:#7fff00><strong>Sample neighbors when doing message passing</strong></span></li></ul></li><li>The graph is <span style=color:red><strong>too large</strong></span><ul><li>$\rightarrow$ <span style=color:#7fff00><strong>Sample subgraphs to compute embedding</strong></span></li></ul></li></ul></li></ul><h3 id=feature-augmentation>Feature augmentation<a hidden class=anchor aria-hidden=true href=#feature-augmentation>#</a></h3><p>Used when graph does not have node features (common when we only have adjacency matrix)</p><h4 id=standard-approaches>Standard approaches<a hidden class=anchor aria-hidden=true href=#standard-approaches>#</a></h4><ul><li>Constant value</li><li>Unique ID to nodes<ul><li>should be converted to one-hot vectors</li></ul></li></ul><h4 id=comparison>Comparison<a hidden class=anchor aria-hidden=true href=#comparison>#</a></h4><p><figure><img src=/img/GNNs/constant_vs_onehot.png alt="constant vs one-hot node features" title="constant vs one-hot node features"><figcaption style="text-align:center;width:75%;margin:10px auto 35px;line-height:.5;color:#888;font-size:15px;font-weight:400">constant vs one-hot node features</figcaption></figure></p><h4 id=-some-structures-are-hard-to-learn-by-gnns>‚ö†Ô∏è Some structures are hard to learn by GNNs<a hidden class=anchor aria-hidden=true href=#-some-structures-are-hard-to-learn-by-gnns>#</a></h4><p>Example: Cycle count feature</p><p><figure><img src=/img/GNNs/cycle_count_feature.png alt="cycle count feature" title="cycle count feature"><figcaption style="text-align:center;width:75%;margin:10px auto 35px;line-height:.5;color:#888;font-size:15px;font-weight:400">cycle count feature</figcaption></figure></p><p>GNN can&rsquo;t learn the length of a cycle that $v_1$ resides in</p><p>Therefore, we should augment node features with cycle count manually by adding a vector of cycle counts like this:</p><p>$$ \text{cycle count feature: } [0, 0, 0, 1, 0, 0] $$</p><p>where index 0 indicates that $v_1$ resides in cycle of length $0$, index 1 indicates cycle of length $1$, and so on.</p><h4 id=commonly-used-augmented-features>Commonly used augmented features<a hidden class=anchor aria-hidden=true href=#commonly-used-augmented-features>#</a></h4><ul><li>Node Degree</li><li>PageRank</li><li>Clustering coefficient</li><li>Node centrality<ul><li>Eigenvector</li><li>Betweenness</li><li>Closeness</li><li>&mldr;</li></ul></li><li>Graphlet</li><li>&mldr;</li></ul><h3 id=add-virtual-nodes--edges>Add virtual nodes / edges<a hidden class=anchor aria-hidden=true href=#add-virtual-nodes--edges>#</a></h3><h4 id=virtual-edge>Virtual edge<a hidden class=anchor aria-hidden=true href=#virtual-edge>#</a></h4><p>Common approach: connect 2-hop neighbors</p><ul><li>instead of using adj. matrix $A$ for GNN computation, use $A^2 + A$</li></ul><p>Use cases:</p><ul><li>Bipartite graphs</li></ul><h4 id=virtual-node>Virtual node<a hidden class=anchor aria-hidden=true href=#virtual-node>#</a></h4><p>Add a virtual node and connect it to other nodes (all or some of them)</p><p>Benefits:</p><ul><li>Greatly <strong>improves message passing in sparse graphs</strong></li></ul><h3 id=node-neighborhood-sampling>Node neighborhood sampling<a hidden class=anchor aria-hidden=true href=#node-neighborhood-sampling>#</a></h3><p>Instead of using all nodes for message passing, (randomly) sample a node&rsquo;s neighborhood. for example, if a node has 5 neighbors, sample 2 of them in the message passing phase.</p><p>Benefits:</p><ul><li>Reduce computational cost<ul><li>Allows for scaling to large graphs</li></ul></li><li>in practice it works great üëå</li></ul><h2 id=prediction-with-gnns>Prediction with GNNs<a hidden class=anchor aria-hidden=true href=#prediction-with-gnns>#</a></h2><h3 id=gnn-training-pipeline>GNN training pipeline<a hidden class=anchor aria-hidden=true href=#gnn-training-pipeline>#</a></h3><p><figure><img src=/img/GNNs/GNN_training_pipeline.png alt="GNN training pipeline" title="GNN training pipeline"><figcaption style="text-align:center;width:75%;margin:10px auto 35px;line-height:.5;color:#888;font-size:15px;font-weight:400">GNN training pipeline</figcaption></figure></p><h4 id=gnn-output>GNN output<a hidden class=anchor aria-hidden=true href=#gnn-output>#</a></h4><p>GNN output is a set of node embeddings:</p><p>$$ \lbrace h_v^{(L)}, \forall v \in G \rbrace $$</p><h3 id=prediction-head>Prediction head<a hidden class=anchor aria-hidden=true href=#prediction-head>#</a></h3><ul><li>Node-level tasks</li><li>Edge-level tasks</li><li>Graph-level tasks</li></ul><h4 id=node-level>Node-level<a hidden class=anchor aria-hidden=true href=#node-level>#</a></h4><p>We can directly use node embeddings or transform them into label space ($y_v$ is ground truth label and $\widehat{y}_v$ is model output):</p><p>$$
\textcolor{Chartreuse}{\widehat{y}_v = h_v^{(L)}}
$$</p><p>$$
\text{or}
$$</p><p>$$
\textcolor{Chartreuse}{\widehat{y}_v = Head_{node}(h_v^{(L)}) = W^{(H)}h_v^{(L)}}
$$</p><h4 id=edge-level>Edge-level<a hidden class=anchor aria-hidden=true href=#edge-level>#</a></h4><p>$$
\color{Chartreuse} \widehat{y}_{uv} = Head_{edge}(h_u^{(L)},h_v^{(L)})
$$</p><p>Options for $Head_{edge}$:</p><ul><li><strong>Concatenation + Linear</strong>: $\text{Linear}(\text{Concat}(h_u^{(L)},h_v^{(L)}))$<ul><li>Similar to graph attention</li></ul></li><li><strong>Dot product</strong>: $(h_u^{(L)})^T h_v^{(L)}$<ul><li>This approach only applies to 1-way prediction</li><li>Applying to $k$-way prediction<ul><li>Similar to multi-head attention: $W^{(1)}, &mldr;, W^{(K)}$ is trainable</li></ul></li></ul></li></ul><p>$$
\widehat{y}_{uv}^{(1)} = (h_u^{(L)})^T \textcolor{red}{W^{(1)}}h_v^{(L)}
$$</p><p>$$
\dots
$$</p><p>$$
\widehat{y}_{uv}^{(K)} = (h_u^{(L)})^T \textcolor{red}{W^{(K)}}h_v^{(L)}
$$</p><p>$$
\widehat{y}_{uv} = \text{Concat}(\widehat{y}_{uv}^{(1)}, \dots, \widehat{y}_{uv}^{(K)}) \in \mathbb{R}^k
$$</p><h4 id=graph-level>Graph-level<a hidden class=anchor aria-hidden=true href=#graph-level>#</a></h4><p>$$
\color{Chartreuse} \widehat{y}_G = Head_{graph}(\lbrace h_v^{(L)} \in \mathbb{R}^d, \forall v \in G \rbrace)
$$</p><p>Options for $Head_{graph}$:</p><ul><li>Global mean pooling</li><li>Global max pooling</li><li>Global sum pooling</li></ul><p>These global pooling methods will work great over small graphs but</p><p>‚ö†Ô∏è Global pooling over a (large) graph will lose information</p><h5 id=hierarchical-global-pooling>Hierarchical global pooling<a hidden class=anchor aria-hidden=true href=#hierarchical-global-pooling>#</a></h5><p>Hierarchically pool section of nodes.</p><p>Which section?</p><p><strong>Diffpool</strong> idea:</p><p><figure><img src=/img/GNNs/hierarchical_pooling.png alt=Diffpool title=Diffpool><figcaption style="text-align:center;width:75%;margin:10px auto 35px;line-height:.5;color:#888;font-size:15px;font-weight:400">Diffpool</figcaption></figure></p><ul><li>Leverage 2 independent GNNs at each level<ul><li><strong>GNN A</strong>: Compute node embeddings</li><li><strong>GNN B</strong>: Compute the cluster that a node belongs to</li></ul></li><li>GNNs A and B at each level can be executed in parallel</li><li>GNNs A and B are trained <em>jointly</em></li></ul><h2 id=training-gnns>Training GNNs<a hidden class=anchor aria-hidden=true href=#training-gnns>#</a></h2><h2 id=setting-up-gnn-prediction-tasks>Setting-up GNN Prediction Tasks<a hidden class=anchor aria-hidden=true href=#setting-up-gnn-prediction-tasks>#</a></h2><h2 id=when-things-dont-go-as-planned>When Things Don&rsquo;t Go As Planned<a hidden class=anchor aria-hidden=true href=#when-things-dont-go-as-planned>#</a></h2><p>The three above topics are almost the same in other deep learning areas so I will put them aside for now.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://alirezanobakht13.github.io/tags/gnn/>GNN</a></li><li><a href=https://alirezanobakht13.github.io/tags/gnns/>GNNs</a></li><li><a href=https://alirezanobakht13.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://alirezanobakht13.github.io/tags/graph-theory/>Graph Theory</a></li><li><a href=https://alirezanobakht13.github.io/tags/gcn/>GCN</a></li><li><a href=https://alirezanobakht13.github.io/tags/graphsage/>GraphSAGE</a></li><li><a href=https://alirezanobakht13.github.io/tags/machine-learning/>Machine Learning</a></li><li><a href=https://alirezanobakht13.github.io/tags/dl/>DL</a></li><li><a href=https://alirezanobakht13.github.io/tags/ml/>ML</a></li></ul><nav class=paginav><a class=next href=https://alirezanobakht13.github.io/posts/test/><span class=title>Next ¬ª</span><br><span>Test</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Graph Neural Networks (GNNs) on twitter" href="https://twitter.com/intent/tweet/?text=Graph%20Neural%20Networks%20%28GNNs%29&amp;url=https%3a%2f%2falirezanobakht13.github.io%2fposts%2fgnns%2f&amp;hashtags=GNN%2cGNNs%2cDeepLearning%2cGraphTheory%2cGCN%2cGraphSAGE%2cMachineLearning%2cDL%2cML"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Graph Neural Networks (GNNs) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2falirezanobakht13.github.io%2fposts%2fgnns%2f&amp;title=Graph%20Neural%20Networks%20%28GNNs%29&amp;summary=Graph%20Neural%20Networks%20%28GNNs%29&amp;source=https%3a%2f%2falirezanobakht13.github.io%2fposts%2fgnns%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Graph Neural Networks (GNNs) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2falirezanobakht13.github.io%2fposts%2fgnns%2f&title=Graph%20Neural%20Networks%20%28GNNs%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Graph Neural Networks (GNNs) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2falirezanobakht13.github.io%2fposts%2fgnns%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Graph Neural Networks (GNNs) on whatsapp" href="https://api.whatsapp.com/send?text=Graph%20Neural%20Networks%20%28GNNs%29%20-%20https%3a%2f%2falirezanobakht13.github.io%2fposts%2fgnns%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Graph Neural Networks (GNNs) on telegram" href="https://telegram.me/share/url?text=Graph%20Neural%20Networks%20%28GNNs%29&amp;url=https%3a%2f%2falirezanobakht13.github.io%2fposts%2fgnns%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://alirezanobakht13.github.io/>Alireza's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>